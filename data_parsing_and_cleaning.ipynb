{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"s7BjO6iOrqdg"},"outputs":[],"source":["#!/usr/bin/env python\n","# coding: utf-8"]},{"cell_type":"markdown","metadata":{"id":"8C8Ho3Norqdj"},"source":["In[ ]:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7pxeuqCrqdm"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"TCTvDtqprqdo","executionInfo":{"status":"ok","timestamp":1643307934441,"user_tz":420,"elapsed":8,"user":{"displayName":"Jacob Minson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisdjvTqZD_3vsySg4gen7LbDKsLdJK53lNEXFHYg=s64","userId":"00481707047547654918"}}},"outputs":[],"source":["# change values in 'CONTROL' column to show more simply what is going on\n","def control_name(file, keep_list):\n","    control_dict = {1:'Public', 2:'Private, NFP',3:'Private, FP', -3:'Unknown'}\n","    new_hd = file.replace({keep_list[keep_list.index('CONTROL')]:control_dict})\n","    return new_hd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bas0UV13rqdr"},"outputs":[],"source":["# clean the 'hd' table containing information about each university, getting only the columns that are relevant.\n","def hdcleaner(year):\n","    import pandas as pd\n","    hd = pd.read_csv('/home/u1033387/FinalProject/histdata/hd{}.csv'.format(year),encoding='ISO-8859-1', index_col=False)\n","    for x in range(len(hd.columns.values)):\n","        hd.columns.values[x] = hd.columns.values[x].upper()\n","    keep_list = ['UNITID','INSTNM', 'STABBR','CONTROL']\n","    \n","    new_hd = hd[keep_list].copy()\n","    new_hd = control_name(new_hd, keep_list)\n","    new_hd['INSTNM'] = new_hd['INSTNM'].str.upper()\n","    \n","    #new_hd = hdegofr1_name(new_hd)\n","    #new_hd = instsize_name(new_hd)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfLlIZVIrqdt"},"outputs":[],"source":["    # clean values and replace zeros with np.nan.  save the output as a .pkl for future calling\n","    new_hd = new_hd.replace(0,np.nan)\n","    new_hd.insert(1, 'YEAR', year)\n","    new_hd.set_index('UNITID')\n","    pklpath = r'/home/u1033387/FinalProject/pickles/'\n","    new_hd.to_pickle(pklpath+'hd{}.pkl'.format(year))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46anJ3MErqdu"},"outputs":[],"source":["# combine different costs including tuition, fees, and books and services into respective columns pertaining to in-state, out-of-state, full-time, and part-time students.\n","def colcompile (file):\n","    new_ic = pd.DataFrame()\n","    new_ic['UNITID'] = file['UNITID']\n","    new_ic['FTIS'] = file['ISUGT']+ file['ISUGF']+ file['BS']\n","    new_ic['FTOS'] = file['OSUGT']+file['OSUGF']+file['BS']\n","    new_ic['PTIS (9CH)'] = (file['ISUGPHC']*9)+file['ISUGF']+file['BS']\n","    new_ic['PTOS (9CH)'] = (file['OSUGPHC']*9)+file['OSUGF']+file['BS']\n","    return new_ic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2kTdLV_rqdw"},"outputs":[],"source":["# some files came with periods in all the blanks, so they are replaced with zeros here and converted to int\n","def nopertoint (file):\n","    file = file.replace('.', 0)\n","    for column in file.columns.values:\n","        file[column] = file[column].astype(int, errors='ignore')\n","    return file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWpx3grQrqdy"},"outputs":[],"source":["# the ic tables contain information about costs of attendance for each school.  Columns are renamed to a readable format and unneccasry columns are done away with.\n","def iccolumns(file, year, keep_list):\n","    iccoldict = {'TUITION2':'ISUGT','FEE2':'ISUGF','HRCHG2':'ISUGPHC','TUITION3':'OSUGT','FEE3':'OSUGF','HRCHG3':'OSUGPHC','CHG4AY2':'BS'}\n","    for column in iccoldict:\n","        for x in range(len(iccoldict)+(len(keep_list)-len(iccoldict))):\n","            if column == file.columns.values[x]:\n","                file.columns.values[x] = iccoldict[column]\n","    return file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijJZKnlYrqd1"},"outputs":[],"source":["# for each year the ic files are cleaned, the above functions are used, and the output is pickled.\n","def iccleaner(year):\n","    import pandas as pd\n","    ic = pd.read_csv('/home/u1033387/FinalProject/histdata/ic{}_ay.csv'.format(year),encoding='utf-8', index_col=False)\n","    for x in range(len(ic.columns.values)):\n","        ic.columns.values[x] = ic.columns.values[x].upper()\n","    keep_list = ['UNITID','TUITION2','FEE2','HRCHG2','TUITION3','FEE3','HRCHG3','CHG4AY2']\n","    ic = ic[keep_list].copy()\n","    \n","    ic = iccolumns(ic, year, keep_list)\n","    ic = nopertoint(ic)\n","    new_ic = colcompile(ic)\n","    new_ic = new_ic.replace(0, np.nan)\n","    new_ic.insert(1, 'YEAR', int(year))\n","    new_ic.set_index('UNITID')\n","#     return new_ic\n","    pklpath = r'/home/u1033387/FinalProject/pickles/'\n","    new_ic.to_pickle(pklpath+'ic{}_ay.pkl'.format(year))\n","    \n","    # this function takes the year format from yyyy1 to yy1yy2 format (ie 2020 becomes 1920, 2015 becomes 1516) \n","def yearconv (year):\n","    iyear = int(year)\n","    year1 = str(iyear-1)\n","    year1 = year1[-2:]\n","    year2 = str(iyear)\n","    year2 = year2[-2:]\n","    iyear = year1+year2\n","    return iyear"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScHqzC63rqd4"},"outputs":[],"source":["# create new column in sfa table (table with grant and loan info) that has the average loan and grant amounts by dividing total contributions by sudents who used contributions\n","def sfamath (file):\n","    file = file.replace(0, np.nan)\n","    file.insert(4, 'UPGRNTA', file['UPGRNTT']/file['UPGRNTN'])\n","    file.insert(7, 'UFLOANA', file['UFLOANT']/file['UFLOANN'])\n","    return file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXn7f6u4rqd6"},"outputs":[],"source":["# the sfa file for 2008 used different column names so they were renamed here\n","def conv2008 (file):\n","    keep_list = ['UNITID','SCUGRAD', 'PGRNT_N', 'PGRNT_A', 'FLOAN_N', 'FLOAN_A']\n","    sfa_dict_08 = {'PGRNT_N':'UPGRNTN', 'PGRNT_A':'UPGRNTA', 'FLOAN_N':'UFLOANN', 'FLOAN_A':'UFLOANA'}\n","    file = file[keep_list].copy()\n","    for column in sfa_dict_08:\n","        for x in range(len(sfa_dict_08)+2):\n","            if column == file.columns.values[x]:\n","                file.columns.values[x] = sfa_dict_08[column]\n","    file = file.replace(0, np.nan)\n","    file.insert(3, 'UPGRNTT', file['UPGRNTA']*file['UPGRNTN'])\n","    file.insert(6, 'UFLOANT', file['UFLOANA']*file['UFLOANN'])\n","    return file\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_10l-_K-rqd8"},"outputs":[],"source":["# 2007 was also an inconsistent format\n","def conv2007 (file):\n","    keep_list = ['UNITID','SCFY2','SCFA2','FGRNT_N','FGRNT_A','LOAN_N','LOAN_A']\n","    sfa_dict_07 = {'FGRNT_N':'UPGRNTN','FGRNT_A':'UPGRNTA','LOAN_N':'UFLOANN','LOAN_A':'UFLOANA'}\n","    file = file[keep_list].copy()\n","    for column in sfa_dict_07:\n","        for x in range(len(sfa_dict_07)+(len(keep_list)-len(sfa_dict_07))):\n","            if column == file.columns.values[x]:\n","                file.columns.values[x] = sfa_dict_07[column]\n","    file = file.replace(0, np.nan)\n","    file.insert(3, 'UPGRNTT', file['UPGRNTA']*file['UPGRNTN'])\n","    file.insert(6, 'UFLOANT', file['UFLOANA']*file['UFLOANN'])\n","    file.insert(1, 'SCUGRAD', file['SCFA2'].fillna(file['SCFA2']))\n","    file.drop(['SCFY2', 'SCFA2'], axis=1, inplace=True)\n","    return file"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"data_parsing_and_cleaning.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}